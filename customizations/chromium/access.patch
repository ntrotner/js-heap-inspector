From bde778f52600ca1c20fbe871ee2032e20ff1a7cc Mon Sep 17 00:00:00 2001
From: Nikita Trotner <nikita@ttnr.me>
Date: Sat, 17 Jan 2026 14:07:10 +0100
Subject: [PATCH] feat: integrate access based tracking

---
 BUILD.bazel                             |   2 +
 BUILD.gn                                |   1 +
 src/execution/isolate-inl.h             |  10 +
 src/execution/isolate.h                 |   3 +
 src/heap/factory.cc                     |  12 +-
 src/heap/heap-object-tracker.cc         | 238 ++++++++++++++++++++++++
 src/heap/heap-object-tracker.h          |  70 +++++++
 src/objects/fixed-array-inl.h           |  33 +++-
 src/objects/heap-object.h               |  10 +-
 src/objects/objects-inl.h               | 101 ++++++++++
 src/objects/string-inl.h                |  20 ++
 src/objects/tagged-field-inl.h          | 133 ++++++++++++-
 src/objects/tagged-field.h              |   9 +
 src/profiler/heap-profiler.cc           |  26 ++-
 src/profiler/heap-profiler.h            |  11 ++
 src/profiler/heap-snapshot-generator.cc |   9 +
 16 files changed, 667 insertions(+), 21 deletions(-)
 create mode 100644 src/heap/heap-object-tracker.cc
 create mode 100644 src/heap/heap-object-tracker.h

diff --git a/BUILD.bazel b/BUILD.bazel
index 4b1f8e19..3773f06a 100644
--- a/BUILD.bazel
+++ b/BUILD.bazel
@@ -1792,6 +1792,8 @@ filegroup(
         "src/heap/heap-layout.cc",
         "src/heap/heap-layout.h",
         "src/heap/heap-layout-inl.h",
+        "src/heap/heap-object-tracker.cc",
+        "src/heap/heap-object-tracker.h",
         "src/heap/heap-layout-tracer.cc",
         "src/heap/heap-layout-tracer.h",
         "src/heap/heap-utils.h",
diff --git a/BUILD.gn b/BUILD.gn
index cfe31c21..811ec3d1 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -5871,6 +5871,7 @@ v8_source_set("v8_base_without_compiler") {
     "src/heap/heap-controller.cc",
     "src/heap/heap-layout-tracer.cc",
     "src/heap/heap-layout.cc",
+    "src/heap/heap-object-tracker.cc",
     "src/heap/heap-verifier.cc",
     "src/heap/heap-visitor.cc",
     "src/heap/heap-write-barrier.cc",
diff --git a/src/execution/isolate-inl.h b/src/execution/isolate-inl.h
index 393b3d61..4c9062f0 100644
--- a/src/execution/isolate-inl.h
+++ b/src/execution/isolate-inl.h
@@ -8,7 +8,9 @@
 #include "src/execution/isolate.h"
 // Include the non-inl header before the rest of the headers.
 
+#include "src/heap/heap-inl.h"
 #include "src/objects/contexts-inl.h"
+#include "src/profiler/heap-profiler.h"
 #include "src/objects/js-function.h"
 #include "src/objects/lookup-inl.h"
 #include "src/objects/objects-inl.h"
@@ -248,6 +250,14 @@ SetCurrentIsolateScope::~SetCurrentIsolateScope() {
   Isolate::SetCurrent(previous_isolate_);
 }
 
+inline HeapProfiler* Isolate::heap_profiler() const {
+  return heap()->heap_profiler();
+}
+
+inline HeapObjectTracker* Isolate::heap_object_tracker() const {
+  return heap_profiler()->object_tracker();
+}
+
 }  // namespace v8::internal
 
 #endif  // V8_EXECUTION_ISOLATE_INL_H_
diff --git a/src/execution/isolate.h b/src/execution/isolate.h
index 88062b1c..1eb816c3 100644
--- a/src/execution/isolate.h
+++ b/src/execution/isolate.h
@@ -137,6 +137,7 @@ class GlobalHandles;
 class GlobalSafepoint;
 class HandleScopeImplementer;
 class HeapObjectToIndexHashMap;
+class HeapObjectTracker;
 class HeapProfiler;
 class InnerPointerToCodeCache;
 class LazyCompileDispatcher;
@@ -1182,6 +1183,8 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   StackGuard* stack_guard() { return isolate_data()->stack_guard(); }
   Heap* heap() { return &heap_; }
   const Heap* heap() const { return &heap_; }
+  HeapProfiler* heap_profiler() const;
+  HeapObjectTracker* heap_object_tracker() const;
   ReadOnlyHeap* read_only_heap() const { return read_only_heap_; }
   static Isolate* FromHeap(const Heap* heap) {
     return reinterpret_cast<Isolate*>(reinterpret_cast<Address>(heap) -
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index d863d8c3..505f59b8 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -26,6 +26,7 @@
 #include "src/heap/heap-allocator-inl.h"
 #include "src/heap/heap-inl.h"
 #include "src/heap/heap-layout-inl.h"
+#include "src/heap/heap-object-tracker.h"
 #include "src/heap/incremental-marking.h"
 #include "src/heap/large-page-inl.h"
 #include "src/heap/mark-compact-inl.h"
@@ -302,8 +303,17 @@ Handle<Code> Factory::CodeBuilder::Build() {
 Tagged<HeapObject> Factory::AllocateRaw(int size, AllocationType allocation,
                                         AllocationAlignment alignment,
                                         AllocationHint hint) {
-  return allocator()->AllocateRawWith<HeapAllocator::kRetryOrFail>(
+  Tagged<HeapObject> result =
+      allocator()->AllocateRawWith<HeapAllocator::kRetryOrFail>(
       size, allocation, AllocationOrigin::kRuntime, alignment, hint);
+
+  // Track allocation using node ID
+  if (V8_UNLIKELY(isolate()->heap_object_tracker() &&
+                  isolate()->heap_object_tracker()->IsProfiling())) {
+    isolate()->heap_object_tracker()->RecordAllocation(result);
+  }
+
+  return result;
 }
 
 Tagged<HeapObject> Factory::AllocateRawWithAllocationSite(
diff --git a/src/heap/heap-object-tracker.cc b/src/heap/heap-object-tracker.cc
new file mode 100644
index 00000000..55762ee9
--- /dev/null
+++ b/src/heap/heap-object-tracker.cc
@@ -0,0 +1,238 @@
+// Copyright 2026 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/heap/heap-object-tracker.h"
+
+#include "src/execution/isolate-inl.h"
+#include "src/handles/handles-inl.h"
+#include "src/heap/heap-inl.h"
+#include "src/profiler/heap-profiler.h"
+#include "src/profiler/output-stream-writer.h"
+
+namespace v8 {
+namespace internal {
+
+HeapObjectTracker::HeapObjectTracker(Isolate* isolate) : isolate_(isolate) {}
+
+HeapObjectTracker::~HeapObjectTracker() {
+  if (is_profiling_.load(std::memory_order_relaxed)) {
+    StopProfiling();
+  }
+}
+
+SnapshotObjectId HeapObjectTracker::GetOrAssignObjectId(
+    Tagged<HeapObject> object) {
+  HeapProfiler* profiler = isolate_->heap_profiler();
+  if (V8_UNLIKELY(profiler == nullptr)) {
+    base::MutexGuard guard(&metrics_mutex_);
+    auto it = address_to_id_.find(object.address());
+    if (it != address_to_id_.end()) {
+      return it->second;
+    }
+    SnapshotObjectId new_id =
+        static_cast<SnapshotObjectId>(address_to_id_.size() + 1);
+    address_to_id_[object.address()] = new_id;
+    return new_id;
+  }
+
+  return profiler->GetOrCreateSnapshotObjectId(object);
+}
+
+void HeapObjectTracker::RecordAllocation(Tagged<HeapObject> object) {
+  if (V8_UNLIKELY(!is_profiling_.load(std::memory_order_relaxed))) return;
+
+  static thread_local bool in_record_access = false;
+  if (in_record_access) return;
+  in_record_access = true;
+
+  SnapshotObjectId id = GetOrAssignObjectId(object);
+  double time_ms = isolate_->time_millis_since_init();
+
+  {
+    base::MutexGuard guard(&metrics_mutex_);
+    auto it = metrics_.find(id);
+    if (it != metrics_.end()) {
+      it->second.allocation_time_ms = time_ms;
+    } else {
+      metrics_[id] = {id, time_ms, 0, 0};
+    }
+  }
+
+  in_record_access = false;
+}
+
+void HeapObjectTracker::RecordLoad(Tagged<HeapObject> object) {
+  if (V8_UNLIKELY(!is_profiling_.load(std::memory_order_relaxed))) return;
+
+  static thread_local bool in_record_access = false;
+  if (in_record_access) return;
+  in_record_access = true;
+
+  static thread_local HeapObjectTracker* last_tracker = nullptr;
+  static thread_local uint32_t last_session_id = 0;
+  static thread_local uint32_t last_gc_count = 0;
+  static thread_local Address last_host = kNullAddress;
+  static thread_local SnapshotObjectId last_id =
+      v8::HeapProfiler::kUnknownObjectId;
+
+  uint32_t current_session_id = session_id_.load(std::memory_order_relaxed);
+  uint32_t current_gc_count = isolate_->heap()->gc_count().value();
+  Address host_addr = object.address();
+  SnapshotObjectId id;
+
+  if (V8_LIKELY(this == last_tracker && host_addr == last_host &&
+                current_session_id == last_session_id &&
+                current_gc_count == last_gc_count)) {
+    id = last_id;
+  } else {
+    HeapProfiler* profiler = isolate_->heap_profiler();
+    if (V8_UNLIKELY(profiler == nullptr)) {
+      in_record_access = false;
+      return;
+    }
+    id = profiler->GetSnapshotObjectId(object);
+    last_tracker = this;
+    last_session_id = current_session_id;
+    last_gc_count = current_gc_count;
+    last_host = host_addr;
+    last_id = id;
+  }
+
+  if (id != v8::HeapProfiler::kUnknownObjectId) {
+    base::MutexGuard guard(&metrics_mutex_);
+    auto it = metrics_.find(id);
+    if (it != metrics_.end()) {
+      it->second.load_count++;
+    } else {
+      metrics_[id] = {id, 0.0, 1, 0};
+    }
+  }
+
+  in_record_access = false;
+}
+
+void HeapObjectTracker::RecordStore(Tagged<HeapObject> object) {
+  if (V8_UNLIKELY(!is_profiling_.load(std::memory_order_relaxed))) return;
+
+  static thread_local bool in_record_access = false;
+  if (in_record_access) return;
+  in_record_access = true;
+
+  static thread_local HeapObjectTracker* last_tracker = nullptr;
+  static thread_local uint32_t last_session_id = 0;
+  static thread_local uint32_t last_gc_count = 0;
+  static thread_local Address last_host = kNullAddress;
+  static thread_local SnapshotObjectId last_id =
+      v8::HeapProfiler::kUnknownObjectId;
+
+  uint32_t current_session_id = session_id_.load(std::memory_order_relaxed);
+  uint32_t current_gc_count = isolate_->heap()->gc_count().value();
+  Address host_addr = object.address();
+  SnapshotObjectId id;
+
+  if (V8_LIKELY(this == last_tracker && host_addr == last_host &&
+                current_session_id == last_session_id &&
+                current_gc_count == last_gc_count)) {
+    id = last_id;
+  } else {
+    HeapProfiler* profiler = isolate_->heap_profiler();
+    if (V8_UNLIKELY(profiler == nullptr)) {
+      in_record_access = false;
+      return;
+    }
+    id = profiler->GetSnapshotObjectId(object);
+    last_tracker = this;
+    last_session_id = current_session_id;
+    last_gc_count = current_gc_count;
+    last_host = host_addr;
+    last_id = id;
+  }
+
+  if (id != v8::HeapProfiler::kUnknownObjectId) {
+    base::MutexGuard guard(&metrics_mutex_);
+    auto it = metrics_.find(id);
+    if (it != metrics_.end()) {
+      it->second.store_count++;
+    } else {
+      metrics_[id] = {id, 0.0, 0, 1};
+    }
+  }
+
+  in_record_access = false;
+}
+
+void HeapObjectTracker::StartProfiling() {
+  base::MutexGuard guard(&metrics_mutex_);
+
+  if (is_profiling_.load(std::memory_order_relaxed)) return;
+
+  session_id_.fetch_add(1, std::memory_order_relaxed);
+  is_profiling_.store(true, std::memory_order_release);
+  metrics_.clear();
+  address_to_id_.clear();
+  isolate_->heap()->AddHeapObjectAllocationTracker(this);
+}
+
+void HeapObjectTracker::StopProfiling() {
+  if (!is_profiling_.load(std::memory_order_relaxed)) return;
+
+  is_profiling_.store(false, std::memory_order_release);
+  isolate_->heap()->RemoveHeapObjectAllocationTracker(this);
+}
+
+bool HeapObjectTracker::HasMetrics() const {
+  base::MutexGuard guard(&metrics_mutex_);
+  return !metrics_.empty();
+}
+
+bool HeapObjectTracker::ExportMetrics(OutputStreamWriter* writer) {
+  base::MutexGuard guard(&metrics_mutex_);
+
+  if (metrics_.empty()) return false;
+
+  writer->AddString("{\"nodes\":[");
+
+  bool first = true;
+  for (const auto& [node_id, metrics] : metrics_) {
+    if (node_id == v8::HeapProfiler::kUnknownObjectId) continue;
+
+    if (!first) writer->AddCharacter(',');
+    first = false;
+
+    writer->AddString("{\"id\":");
+    writer->AddNumber(node_id);
+    writer->AddString(",\"allocation_time_ms\":");
+
+    char buffer[64];
+    base::OS::SNPrintF(buffer, sizeof(buffer), "%.3f",
+                       metrics.allocation_time_ms);
+    writer->AddString(buffer);
+
+    writer->AddString(",\"load_count\":");
+    writer->AddNumber(metrics.load_count);
+    writer->AddString(",\"store_count\":");
+    writer->AddNumber(metrics.store_count);
+    writer->AddCharacter('}');
+  }
+
+  writer->AddString("]}");
+  return true;
+}
+
+void HeapObjectTracker::AllocationEvent(Address addr, int size) {
+  // We already track allocations via Factory::AllocateRaw for better accuracy.
+}
+
+void HeapObjectTracker::MoveEvent(Address from, Address to, int size) {
+  base::MutexGuard guard(&metrics_mutex_);
+  auto it = address_to_id_.find(from);
+  if (it != address_to_id_.end()) {
+    SnapshotObjectId id = it->second;
+    address_to_id_.erase(it);
+    address_to_id_[to] = id;
+  }
+}
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/heap/heap-object-tracker.h b/src/heap/heap-object-tracker.h
new file mode 100644
index 00000000..7afc3a99
--- /dev/null
+++ b/src/heap/heap-object-tracker.h
@@ -0,0 +1,70 @@
+// Copyright 2026 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_HEAP_HEAP_OBJECT_TRACKER_H_
+#define V8_HEAP_HEAP_OBJECT_TRACKER_H_
+
+#include <atomic>
+#include <unordered_map>
+
+#include "include/v8-profiler.h"
+#include "src/base/macros.h"
+#include "src/base/platform/mutex.h"
+#include "src/common/globals.h"
+#include "src/heap/heap.h"
+
+namespace v8 {
+namespace internal {
+
+class Isolate;
+class OutputStreamWriter;
+
+struct ObjectAccessMetrics {
+  SnapshotObjectId node_id;
+  double allocation_time_ms;
+  uint64_t load_count;
+  uint64_t store_count;
+};
+
+class V8_EXPORT_PRIVATE HeapObjectTracker : public HeapObjectAllocationTracker {
+ public:
+  explicit HeapObjectTracker(Isolate* isolate);
+  ~HeapObjectTracker() override;
+
+  HeapObjectTracker(const HeapObjectTracker&) = delete;
+  HeapObjectTracker& operator=(const HeapObjectTracker&) = delete;
+
+  void RecordAllocation(Tagged<HeapObject> object);
+  void RecordLoad(Tagged<HeapObject> object);
+  void RecordStore(Tagged<HeapObject> object);
+
+  void StartProfiling();
+  void StopProfiling();
+  bool IsProfiling() const {
+    return is_profiling_.load(std::memory_order_relaxed);
+  }
+
+  bool ExportMetrics(OutputStreamWriter* writer);
+  bool HasMetrics() const;
+
+  // HeapObjectAllocationTracker implementation
+  void AllocationEvent(Address addr, int size) override;
+  void MoveEvent(Address from, Address to, int size) override;
+
+ private:
+  SnapshotObjectId GetOrAssignObjectId(Tagged<HeapObject> object);
+
+  Isolate* isolate_;
+  std::atomic<bool> is_profiling_{false};
+  std::atomic<uint32_t> session_id_{0};
+  mutable base::Mutex metrics_mutex_;
+
+  std::unordered_map<SnapshotObjectId, ObjectAccessMetrics> metrics_;
+  std::unordered_map<Address, SnapshotObjectId> address_to_id_;
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_HEAP_HEAP_OBJECT_TRACKER_H_
diff --git a/src/objects/fixed-array-inl.h b/src/objects/fixed-array-inl.h
index da4013a9..3ad99d55 100644
--- a/src/objects/fixed-array-inl.h
+++ b/src/objects/fixed-array-inl.h
@@ -17,6 +17,7 @@
 #include "src/numbers/conversions.h"
 #include "src/objects/bigint.h"
 #include "src/objects/compressed-slots.h"
+#include "src/heap/heap-object-tracker.h"
 #include "src/objects/hole.h"
 #include "src/objects/map.h"
 #include "src/objects/maybe-object-inl.h"
@@ -125,28 +126,28 @@ Tagged<typename TaggedArrayBase<D, S, P>::ElementT>
 TaggedArrayBase<D, S, P>::get(uint32_t index) const {
   DCHECK(IsInBounds(index));
   // TODO(jgruber): This tag-less overload shouldn't be relaxed.
-  return objects()[index].Relaxed_Load();
+  return objects()[index].Relaxed_Load(this);
 }
 
 template <class D, class S, class P>
 Tagged<typename TaggedArrayBase<D, S, P>::ElementT>
 TaggedArrayBase<D, S, P>::get(uint32_t index, RelaxedLoadTag) const {
   DCHECK(IsInBounds(index));
-  return objects()[index].Relaxed_Load();
+  return objects()[index].Relaxed_Load(this);
 }
 
 template <class D, class S, class P>
 Tagged<typename TaggedArrayBase<D, S, P>::ElementT>
 TaggedArrayBase<D, S, P>::get(uint32_t index, AcquireLoadTag) const {
   DCHECK(IsInBounds(index));
-  return objects()[index].Acquire_Load();
+  return objects()[index].Acquire_Load(this);
 }
 
 template <class D, class S, class P>
 Tagged<typename TaggedArrayBase<D, S, P>::ElementT>
 TaggedArrayBase<D, S, P>::get(uint32_t index, SeqCstAccessTag) const {
   DCHECK(IsInBounds(index));
-  return objects()[index].SeqCst_Load();
+  return objects()[index].SeqCst_Load(this);
 }
 
 template <class D, class S, class P>
@@ -614,6 +615,12 @@ Handle<D> PrimitiveArrayBase<D, S, P>::Allocate(
 
 double FixedDoubleArray::get_scalar(uint32_t index) {
   DCHECK(!is_the_hole(index));
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(this));
+  }
   return values()[index].value();
 }
 
@@ -639,6 +646,12 @@ void FixedDoubleArray::set(uint32_t index, double value) {
   if (std::isnan(value)) {
     value = std::numeric_limits<double>::quiet_NaN();
   }
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(this));
+  }
   values()[index].set_value(value);
   DCHECK(!is_the_hole(index));
 }
@@ -915,12 +928,24 @@ Address FixedIntegerArrayBase<T, Base>::get_element_address(int index) const {
 template <typename T, typename Base>
 T FixedIntegerArrayBase<T, Base>::get(int index) const {
   static_assert(std::is_integral_v<T>);
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(this));
+  }
   return base::ReadUnalignedValue<T>(get_element_address(index));
 }
 
 template <typename T, typename Base>
 void FixedIntegerArrayBase<T, Base>::set(int index, T value) {
   static_assert(std::is_integral_v<T>);
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(this));
+  }
   return base::WriteUnalignedValue<T>(get_element_address(index), value);
 }
 
diff --git a/src/objects/heap-object.h b/src/objects/heap-object.h
index 5356ca25..8c9e104a 100644
--- a/src/objects/heap-object.h
+++ b/src/objects/heap-object.h
@@ -258,18 +258,12 @@ class HeapObject : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   template <class T>
   inline T ReadField(size_t offset) const
     requires(std::is_arithmetic_v<T> || std::is_enum_v<T> ||
-             std::is_pointer_v<T>)
-  {
-    return ReadMaybeUnalignedValue<T>(field_address(offset));
-  }
+             std::is_pointer_v<T>);
 
   template <class T>
   inline void WriteField(size_t offset, T value) const
     requires(std::is_arithmetic_v<T> || std::is_enum_v<T> ||
-             std::is_pointer_v<T>)
-  {
-    return WriteMaybeUnalignedValue<T>(field_address(offset), value);
-  }
+             std::is_pointer_v<T>);
 
   // Atomically reads a field using relaxed memory ordering. Can only be used
   // with integral types whose size is <= kTaggedSize (to guarantee alignment).
diff --git a/src/objects/objects-inl.h b/src/objects/objects-inl.h
index ede516bc..bdca1807 100644
--- a/src/objects/objects-inl.h
+++ b/src/objects/objects-inl.h
@@ -33,6 +33,7 @@
 #include "src/objects/casting.h"
 #include "src/objects/deoptimization-data.h"
 #include "src/objects/heap-number-inl.h"
+#include "src/heap/heap-object-tracker.h"
 #include "src/objects/heap-object.h"
 #include "src/objects/hole.h"
 #include "src/objects/instance-type-checker.h"
@@ -388,11 +389,45 @@ template <>
 struct CastTraits<DeoptimizationFrameTranslation>
     : public CastTraits<TrustedByteArray> {};
 
+template <class T>
+T HeapObject::ReadField(size_t offset) const
+  requires(std::is_arithmetic_v<T> || std::is_enum_v<T> ||
+           std::is_pointer_v<T>)
+{
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(*this));
+  }
+  return ReadMaybeUnalignedValue<T>(field_address(offset));
+}
+
+template <class T>
+void HeapObject::WriteField(size_t offset, T value) const
+  requires(std::is_arithmetic_v<T> || std::is_enum_v<T> ||
+           std::is_pointer_v<T>)
+{
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(*this));
+  }
+  return WriteMaybeUnalignedValue<T>(field_address(offset), value);
+}
+
 template <class T>
 T HeapObject::Relaxed_ReadField(size_t offset) const
   requires((std::is_arithmetic_v<T> || std::is_enum_v<T>) &&
            !std::is_floating_point_v<T>)
 {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(*this));
+  }
   // Pointer compression causes types larger than kTaggedSize to be
   // unaligned. Atomic loads must be aligned.
   DCHECK_IMPLIES(COMPRESS_POINTERS_BOOL, sizeof(T) <= kTaggedSize);
@@ -406,6 +441,12 @@ void HeapObject::Relaxed_WriteField(size_t offset, T value)
   requires((std::is_arithmetic_v<T> || std::is_enum_v<T>) &&
            !std::is_floating_point_v<T>)
 {
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(*this));
+  }
   // Pointer compression causes types larger than kTaggedSize to be
   // unaligned. Atomic stores must be aligned.
   DCHECK_IMPLIES(COMPRESS_POINTERS_BOOL, sizeof(T) <= kTaggedSize);
@@ -420,6 +461,12 @@ T HeapObject::Acquire_ReadField(size_t offset) const
   requires((std::is_arithmetic_v<T> || std::is_enum_v<T>) &&
            !std::is_floating_point_v<T>)
 {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(*this));
+  }
   // Pointer compression causes types larger than kTaggedSize to be
   // unaligned. Atomic loads must be aligned.
   DCHECK_IMPLIES(COMPRESS_POINTERS_BOOL, sizeof(T) <= kTaggedSize);
@@ -1057,26 +1104,56 @@ MaybeDirectHandle<Object> Object::SetElement(Isolate* isolate,
 
 Address HeapObject::ReadSandboxedPointerField(
     size_t offset, PtrComprCageBase cage_base) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(*this));
+  }
   return i::ReadSandboxedPointerField(field_address(offset), cage_base);
 }
 
 void HeapObject::WriteSandboxedPointerField(size_t offset,
                                             PtrComprCageBase cage_base,
                                             Address value) {
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(*this));
+  }
   i::WriteSandboxedPointerField(field_address(offset), cage_base, value);
 }
 
 void HeapObject::WriteSandboxedPointerField(size_t offset, Isolate* isolate,
                                             Address value) {
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(*this));
+  }
   i::WriteSandboxedPointerField(field_address(offset),
                                 PtrComprCageBase(isolate), value);
 }
 
 size_t HeapObject::ReadBoundedSizeField(size_t offset) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(*this));
+  }
   return i::ReadBoundedSizeField(field_address(offset));
 }
 
 void HeapObject::WriteBoundedSizeField(size_t offset, size_t value) {
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(*this));
+  }
   i::WriteBoundedSizeField(field_address(offset), value);
 }
 
@@ -1104,12 +1181,24 @@ void HeapObject::InitExternalPointerField(size_t offset,
 template <ExternalPointerTagRange tag_range>
 Address HeapObject::ReadExternalPointerField(size_t offset,
                                              IsolateForSandbox isolate) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(*this));
+  }
   return i::ReadExternalPointerField<tag_range>(field_address(offset), isolate);
 }
 
 Address HeapObject::ReadExternalPointerField(
     size_t offset, IsolateForSandbox isolate,
     ExternalPointerTagRange tag_range) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(*this));
+  }
   return i::ReadExternalPointerField(field_address(offset), isolate, tag_range);
 }
 
@@ -1130,6 +1219,12 @@ template <ExternalPointerTag tag>
 void HeapObject::WriteExternalPointerField(size_t offset,
                                            IsolateForSandbox isolate,
                                            Address value) {
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(*this));
+  }
   i::WriteExternalPointerField<tag>(field_address(offset), isolate, value);
 }
 
@@ -1137,6 +1232,12 @@ void HeapObject::WriteExternalPointerField(size_t offset,
                                            IsolateForSandbox isolate,
                                            ExternalPointerTag tag,
                                            Address value) {
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(*this));
+  }
   i::WriteExternalPointerField(field_address(offset), isolate, tag, value);
 }
 
diff --git a/src/objects/string-inl.h b/src/objects/string-inl.h
index 829d6781..c2d38b52 100644
--- a/src/objects/string-inl.h
+++ b/src/objects/string-inl.h
@@ -20,6 +20,7 @@
 #include "src/heap/factory.h"
 #include "src/heap/heap-layout-inl.h"
 #include "src/numbers/hash-seed-inl.h"
+#include "src/heap/heap-object-tracker.h"
 #include "src/objects/heap-object.h"
 #include "src/objects/instance-type-checker.h"
 #include "src/objects/instance-type-inl.h"
@@ -1128,6 +1129,13 @@ uint16_t String::GetImpl(
     uint32_t index, const SharedStringAccessGuardIfNeeded& access_guard) const {
   DCHECK(index >= 0 && index < length());
 
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(this));
+  }
+
   return DispatchToSpecificType(
       [&](auto str) { return str->Get(index, access_guard); });
 }
@@ -1299,6 +1307,12 @@ void SeqOneByteString::SeqOneByteStringSet(uint32_t index, uint16_t value) {
   DCHECK_GE(index, 0);
   DCHECK_LT(index, length());
   DCHECK_LE(value, kMaxOneByteCharCode);
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(this));
+  }
   chars()[index] = value;
 }
 
@@ -1357,6 +1371,12 @@ uint16_t SeqTwoByteString::Get(
 void SeqTwoByteString::SeqTwoByteStringSet(uint32_t index, uint16_t value) {
   DisallowGarbageCollection no_gc;
   DCHECK(index >= 0 && index < length());
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(this));
+  }
   chars()[index] = value;
 }
 
diff --git a/src/objects/tagged-field-inl.h b/src/objects/tagged-field-inl.h
index 7ff9b9ba..b9149e92 100644
--- a/src/objects/tagged-field-inl.h
+++ b/src/objects/tagged-field-inl.h
@@ -12,6 +12,7 @@
 
 #include "src/common/globals.h"
 #include "src/common/ptr-compr-inl.h"
+#include "src/heap/heap-object-tracker.h"
 #include "src/heap/heap-write-barrier-inl.h"
 #include "src/objects/tagged.h"
 
@@ -56,11 +57,30 @@ Tagged<T> TaggedMember<T, CompressionScheme>::load() const {
 }
 
 template <typename T, typename CompressionScheme>
-void TaggedMember<T, CompressionScheme>::store(HeapObjectLayout* host,
+inline Tagged<T> TaggedMember<T, CompressionScheme>::load(
+    const HeapObjectLayout* host) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(const_cast<HeapObjectLayout*>(host)));
+  }
+  return load();
+}
+
+template <typename T, typename CompressionScheme>
+inline void TaggedMember<T, CompressionScheme>::store(HeapObjectLayout* host,
                                                Tagged<T> value,
                                                WriteBarrierMode mode) {
   store_no_write_barrier(value);
   WriteBarrier(host, value, mode);
+
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(host));
+  }
 }
 
 template <typename T, typename CompressionScheme>
@@ -70,11 +90,30 @@ Tagged<T> TaggedMember<T, CompressionScheme>::Relaxed_Load() const {
 }
 
 template <typename T, typename CompressionScheme>
-void TaggedMember<T, CompressionScheme>::Relaxed_Store(HeapObjectLayout* host,
+inline Tagged<T> TaggedMember<T, CompressionScheme>::Relaxed_Load(
+    const HeapObjectLayout* host) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(const_cast<HeapObjectLayout*>(host)));
+  }
+  return Relaxed_Load();
+}
+
+template <typename T, typename CompressionScheme>
+inline void TaggedMember<T, CompressionScheme>::Relaxed_Store(HeapObjectLayout* host,
                                                        Tagged<T> value,
                                                        WriteBarrierMode mode) {
   Relaxed_Store_no_write_barrier(value);
   WriteBarrier(host, value, mode);
+
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(host));
+  }
 }
 
 template <typename T, typename CompressionScheme>
@@ -84,11 +123,30 @@ Tagged<T> TaggedMember<T, CompressionScheme>::Acquire_Load() const {
 }
 
 template <typename T, typename CompressionScheme>
-void TaggedMember<T, CompressionScheme>::Release_Store(HeapObjectLayout* host,
+inline Tagged<T> TaggedMember<T, CompressionScheme>::Acquire_Load(
+    const HeapObjectLayout* host) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(const_cast<HeapObjectLayout*>(host)));
+  }
+  return Acquire_Load();
+}
+
+template <typename T, typename CompressionScheme>
+inline void TaggedMember<T, CompressionScheme>::Release_Store(HeapObjectLayout* host,
                                                        Tagged<T> value,
                                                        WriteBarrierMode mode) {
   Release_Store_no_write_barrier(value);
   WriteBarrier(host, value, mode);
+
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(host));
+  }
 }
 
 template <typename T, typename CompressionScheme>
@@ -98,24 +156,50 @@ Tagged<T> TaggedMember<T, CompressionScheme>::SeqCst_Load() const {
 }
 
 template <typename T, typename CompressionScheme>
-void TaggedMember<T, CompressionScheme>::SeqCst_Store(HeapObjectLayout* host,
+inline Tagged<T> TaggedMember<T, CompressionScheme>::SeqCst_Load(
+    const HeapObjectLayout* host) const {
+  // Track load by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(
+        Tagged<HeapObject>(const_cast<HeapObjectLayout*>(host)));
+  }
+  return SeqCst_Load();
+}
+
+template <typename T, typename CompressionScheme>
+inline void TaggedMember<T, CompressionScheme>::SeqCst_Store(HeapObjectLayout* host,
                                                       Tagged<T> value,
                                                       WriteBarrierMode mode) {
   SeqCst_Store_no_write_barrier(value);
   WriteBarrier(host, value, mode);
+
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(host));
+  }
 }
 
 template <typename T, typename CompressionScheme>
-Tagged<T> TaggedMember<T, CompressionScheme>::SeqCst_Swap(
+inline Tagged<T> TaggedMember<T, CompressionScheme>::SeqCst_Swap(
     HeapObjectLayout* host, Tagged<T> value, WriteBarrierMode mode) {
   Tagged<T> old_value(tagged_to_full(AsAtomicTagged::SeqCst_Swap(
       this->ptr_location(), full_to_tagged(value.ptr()))));
   WriteBarrier(host, value, mode);
+
+  // Track store by node ID
+  if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(
+        Tagged<HeapObject>(host));
+  }
   return old_value;
 }
 
 template <typename T, typename CompressionScheme>
-Tagged<T> TaggedMember<T, CompressionScheme>::SeqCst_CompareAndSwap(
+inline Tagged<T> TaggedMember<T, CompressionScheme>::SeqCst_CompareAndSwap(
     HeapObjectLayout* host, Tagged<T> expected_value, Tagged<T> value,
     WriteBarrierMode mode) {
   Tagged<T> old_value(tagged_to_full(AsAtomicTagged::SeqCst_CompareAndSwap(
@@ -123,6 +207,13 @@ Tagged<T> TaggedMember<T, CompressionScheme>::SeqCst_CompareAndSwap(
       full_to_tagged(value.ptr()))));
   if (old_value == expected_value) {
     WriteBarrier(host, value, mode);
+
+    // Track store by node ID
+    if (V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                    Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+      Isolate::Current()->heap_object_tracker()->RecordStore(
+          Tagged<HeapObject>(host));
+    }
   }
   return old_value;
 }
@@ -223,6 +314,14 @@ TaggedField<T, kFieldOffset, CompressionScheme>::load(Tagged<HeapObject> host,
                                                       int offset) {
   Tagged_t value = *location(host, offset);
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
+
+  // Track load by node ID
+  if (kFieldOffset + offset != HeapObject::kMapOffset &&
+      V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(host);
+  }
+
   return PtrType(tagged_to_full(host.ptr(), value));
 }
 
@@ -233,6 +332,14 @@ TaggedField<T, kFieldOffset, CompressionScheme>::load(
     PtrComprCageBase cage_base, Tagged<HeapObject> host, int offset) {
   Tagged_t value = *location(host, offset);
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
+
+  // Track load by node ID
+  if (kFieldOffset + offset != HeapObject::kMapOffset &&
+      V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordLoad(host);
+  }
+
   return PtrType(tagged_to_full(cage_base, value));
 }
 
@@ -247,6 +354,13 @@ void TaggedField<T, kFieldOffset, CompressionScheme>::store(
   DCHECK_NE(kFieldOffset, HeapObject::kMapOffset);
   *location(host) = full_to_tagged(ptr);
 #endif
+
+  // Track store by node ID
+  if (kFieldOffset != HeapObject::kMapOffset &&
+      V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(host);
+  }
 }
 
 // static
@@ -260,6 +374,13 @@ void TaggedField<T, kFieldOffset, CompressionScheme>::store(
   DCHECK_NE(kFieldOffset + offset, HeapObject::kMapOffset);
   *location(host, offset) = full_to_tagged(ptr);
 #endif
+
+  // Track store by node ID
+  if (kFieldOffset + offset != HeapObject::kMapOffset &&
+      V8_UNLIKELY(Isolate::Current()->heap_object_tracker() &&
+                  Isolate::Current()->heap_object_tracker()->IsProfiling())) {
+    Isolate::Current()->heap_object_tracker()->RecordStore(host);
+  }
 }
 
 // static
diff --git a/src/objects/tagged-field.h b/src/objects/tagged-field.h
index 4b7722c5..2e87ea30 100644
--- a/src/objects/tagged-field.h
+++ b/src/objects/tagged-field.h
@@ -21,6 +21,11 @@ namespace v8::internal {
 template <typename T, typename CompressionScheme = V8HeapCompressionScheme>
 class TaggedMember;
 
+template <typename T>
+class Tagged;
+
+class HeapObjectLayout;
+
 template <typename T>
 using ProtectedTaggedMember = TaggedMember<T, TrustedSpaceCompressionScheme>;
 
@@ -35,21 +40,25 @@ class TaggedMember : public TaggedMemberBase {
   constexpr TaggedMember() = default;
 
   inline Tagged<T> load() const;
+  inline Tagged<T> load(const HeapObjectLayout* host) const;
   inline void store(HeapObjectLayout* host, Tagged<T> value,
                     WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   inline void store_no_write_barrier(Tagged<T> value);
 
   inline Tagged<T> Relaxed_Load() const;
+  inline Tagged<T> Relaxed_Load(const HeapObjectLayout* host) const;
   inline void Relaxed_Store(HeapObjectLayout* host, Tagged<T> value,
                             WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   inline void Relaxed_Store_no_write_barrier(Tagged<T> value);
 
   inline Tagged<T> Acquire_Load() const;
+  inline Tagged<T> Acquire_Load(const HeapObjectLayout* host) const;
   inline void Release_Store(HeapObjectLayout* host, Tagged<T> value,
                             WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   inline void Release_Store_no_write_barrier(Tagged<T> value);
 
   inline Tagged<T> SeqCst_Load() const;
+  inline Tagged<T> SeqCst_Load(const HeapObjectLayout* host) const;
   inline void SeqCst_Store(HeapObjectLayout* host, Tagged<T> value,
                            WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   inline void SeqCst_Store_no_write_barrier(Tagged<T> value);
diff --git a/src/profiler/heap-profiler.cc b/src/profiler/heap-profiler.cc
index c123645e..e0cb50d3 100644
--- a/src/profiler/heap-profiler.cc
+++ b/src/profiler/heap-profiler.cc
@@ -15,6 +15,7 @@
 #include "src/heap/combined-heap.h"
 #include "src/heap/heap-inl.h"
 #include "src/heap/heap-layout-inl.h"
+#include "src/heap/heap-object-tracker.h"
 #include "src/heap/heap.h"
 #include "src/objects/cpp-heap-object-wrapper-inl.h"
 #include "src/objects/js-array-buffer-inl.h"
@@ -261,6 +262,12 @@ void HeapProfiler::StartHeapObjectsTracking(bool track_allocations) {
   if (track_allocations) {
     allocation_tracker_.reset(new AllocationTracker(ids_.get(), names_.get()));
     heap()->AddHeapObjectAllocationTracker(this);
+
+    // Also start object access tracking when allocation timeline is requested
+    if (!object_tracker_) {
+      object_tracker_.reset(new HeapObjectTracker(heap()->isolate()));
+    }
+    object_tracker_->StartProfiling();
   }
 }
 
@@ -276,6 +283,9 @@ void HeapProfiler::StopHeapObjectsTracking() {
     MaybeClearStringsStorage();
     heap()->RemoveHeapObjectAllocationTracker(this);
   }
+  if (object_tracker_) {
+    object_tracker_->StopProfiling();
+  }
 }
 
 int HeapProfiler::GetSnapshotsCount() const {
@@ -289,11 +299,23 @@ HeapSnapshot* HeapProfiler::GetSnapshot(int index) {
 }
 
 SnapshotObjectId HeapProfiler::GetSnapshotObjectId(DirectHandle<Object> obj) {
-  if (!IsHeapObject(*obj)) return v8::HeapProfiler::kUnknownObjectId;
-  return ids_->FindEntry(Cast<HeapObject>(*obj).address());
+  return GetSnapshotObjectId(*obj);
+}
+
+SnapshotObjectId HeapProfiler::GetSnapshotObjectId(Tagged<Object> obj) {
+  if (!IsHeapObject(obj)) return v8::HeapProfiler::kUnknownObjectId;
+  base::MutexGuard guard(&profiler_mutex_);
+  return ids_->FindEntry(Cast<HeapObject>(obj).address());
+}
+
+SnapshotObjectId HeapProfiler::GetOrCreateSnapshotObjectId(Tagged<HeapObject> obj) {
+  base::MutexGuard guard(&profiler_mutex_);
+  return ids_->FindOrAddEntry(obj.address(), obj->Size(isolate()),
+                               HeapObjectsMap::MarkEntryAccessed::kNo);
 }
 
 SnapshotObjectId HeapProfiler::GetSnapshotObjectId(NativeObject obj) {
+  base::MutexGuard guard(&profiler_mutex_);
   // Try to find id of regular native node first.
   SnapshotObjectId id = ids_->FindEntry(reinterpret_cast<Address>(obj));
   // In case no id has been found, check whether there exists an entry where the
diff --git a/src/profiler/heap-profiler.h b/src/profiler/heap-profiler.h
index 82d4db26..2dbaedd9 100644
--- a/src/profiler/heap-profiler.h
+++ b/src/profiler/heap-profiler.h
@@ -19,6 +19,7 @@ namespace v8 {
 namespace internal {
 
 // Forward declarations.
+class HeapObjectTracker;
 class AllocationTracker;
 class HeapObjectsMap;
 class HeapProfiler;
@@ -79,8 +80,13 @@ class HeapProfiler : public HeapObjectAllocationTracker {
   bool is_sampling_allocations() { return !!sampling_heap_profiler_; }
   AllocationProfile* GetAllocationProfile();
 
+  // Allocation timeline tracking
   void StartHeapObjectsTracking(bool track_allocations);
   void StopHeapObjectsTracking();
+
+  // Access tracking integration - automatically starts/stops with allocation timeline
+  HeapObjectTracker* object_tracker() { return object_tracker_.get(); }
+
   AllocationTracker* allocation_tracker() const {
     return allocation_tracker_.get();
   }
@@ -93,6 +99,8 @@ class HeapProfiler : public HeapObjectAllocationTracker {
   bool IsTakingSnapshot() const;
   HeapSnapshot* GetSnapshot(int index);
   SnapshotObjectId GetSnapshotObjectId(DirectHandle<Object> obj);
+  SnapshotObjectId GetSnapshotObjectId(Tagged<Object> obj);
+  SnapshotObjectId GetOrCreateSnapshotObjectId(Tagged<HeapObject> obj);
   SnapshotObjectId GetSnapshotObjectId(NativeObject obj);
   void DeleteAllSnapshots();
   void RemoveSnapshot(HeapSnapshot* snapshot);
@@ -176,6 +184,9 @@ class HeapProfiler : public HeapObjectAllocationTracker {
   std::pair<v8::HeapProfiler::GetDetachednessCallback, void*>
       get_detachedness_callback_;
   std::unique_ptr<HeapProfilerNativeMoveListener> native_move_listener_;
+
+  // Owned access tracker - lifecycle tied to allocation timeline profiling
+  std::unique_ptr<HeapObjectTracker> object_tracker_;
 };
 
 }  // namespace internal
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index 44f4535a..ab219a5d 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -16,6 +16,7 @@
 #include "src/handles/global-handles.h"
 #include "src/heap/combined-heap.h"
 #include "src/heap/heap-layout-inl.h"
+#include "src/heap/heap-object-tracker.h"
 #include "src/heap/heap.h"
 #include "src/heap/safepoint.h"
 #include "src/heap/visit-object.h"
@@ -3547,6 +3548,14 @@ void HeapSnapshotJSONSerializer::SerializeImpl() {
   if (writer_->aborted()) return;
   writer_->AddString("],\n");
 
+  if (HeapObjectTracker* tracker = snapshot_->profiler()->object_tracker()) {
+    if (tracker->HasMetrics()) {
+      writer_->AddString("\"metrics\":");
+      tracker->ExportMetrics(writer_);
+      writer_->AddString(",\n");
+    }
+  }
+
   writer_->AddString("\"strings\":[");
   SerializeStrings();
   if (writer_->aborted()) return;
-- 
2.52.0

